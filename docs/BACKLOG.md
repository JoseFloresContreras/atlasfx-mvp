# AtlasFX - Backlog & Future Roadmap

**Version:** 1.0  
**Last Updated:** 20 de Octubre, 2025  
**Status:** Planning

---

## Purpose

Este documento captura ideas, mejoras y caracterÃ­sticas planificadas para versiones futuras del MVP y fases posteriores del proyecto AtlasFX. Mantiene un registro organizado de todo lo que se quiere implementar pero que estÃ¡ fuera del alcance inmediato del MVP actual.

---

## CÃ³mo Usar Este Backlog

### CategorÃ­as de Prioridad

- **ğŸ”¥ Alta Prioridad (Post-MVP Inmediato):** CaracterÃ­sticas crÃ­ticas para pasar de MVP a producciÃ³n
- **â­ Media Prioridad (VersiÃ³n 2.0):** Mejoras importantes pero no bloqueantes
- **ğŸ’¡ Baja Prioridad (Futuro):** Ideas interesantes para explorar eventualmente
- **ğŸ”¬ InvestigaciÃ³n:** Requiere mÃ¡s investigaciÃ³n antes de decidir implementaciÃ³n

### Estados

- **ğŸ“‹ Backlog:** Idea capturada, no iniciada
- **ğŸ” En AnÃ¡lisis:** Evaluando viabilidad y diseÃ±o
- **âœ… Completado:** Implementado y desplegado
- **âŒ Descartado:** Decidido no implementar (con razÃ³n documentada)

---

## MVP Current Scope (Baseline)

### Core Components (En Desarrollo)

- **Data Pipeline:** Procesamiento de tick data Level 1 (7 pares forex)
- **VAE:** RepresentaciÃ³n latente del estado de mercado
- **TFT:** PronÃ³sticos multi-horizonte con incertidumbre
- **SAC:** Agente de trading con polÃ­tica estocÃ¡stica
- **Backtesting:** Sistema de evaluaciÃ³n en datos histÃ³ricos

### MVP Features

- 7 pares de forex (EURUSD, GBPUSD, USDJPY, AUDUSD, USDCAD, USDCHF, NZDUSD)
- Ventanas temporales: 1-10 minutos
- Horizones de predicciÃ³n: 1, 5, 10 minutos
- MÃ©tricas: Sharpe ratio, drawdown, win rate, profit factor
- Reproducibilidad: Seeds fijos, DVC, MLflow

---

## ğŸ”¥ Post-MVP Inmediato (Prioridad Alta)

### 1. ExpansiÃ³n de Instrumentos

**Status:** ğŸ“‹ Backlog  
**CategorÃ­a:** Datos  
**Esfuerzo Estimado:** 2-3 semanas

**DescripciÃ³n:**
Agregar 3 instrumentos adicionales mencionados en el problema statement original.

**Candidatos:**
- **Commodities:** Gold (XAU/USD), Oil (WTI)
- **Ãndices:** S&P 500 futures (ES)
- **Crypto:** Bitcoin (BTC/USD) - si disponible en Dukascopy

**RazÃ³n:**
- DiversificaciÃ³n de portafolio
- ReducciÃ³n de correlaciÃ³n entre assets
- Mejor gestiÃ³n de riesgo

**ImplementaciÃ³n:**
1. Extender pipeline de datos para soportar mÃºltiples tipos de instrumentos
2. Ajustar features (algunos no aplicables a todos los instrumentos)
3. Actualizar VAE para manejar caracterÃ­sticas especÃ­ficas por instrumento
4. Entrenar modelos multi-asset

**Consideraciones:**
- Â¿Usar un solo modelo o modelos especializados por tipo de asset?
- Â¿CÃ³mo normalizar diferentes escalas de precios?
- Â¿Distintas estructuras de costos de transacciÃ³n?

---

### 2. Slippage y Market Impact Models

**Status:** ğŸ“‹ Backlog  
**CategorÃ­a:** Trading Environment  
**Esfuerzo Estimado:** 1-2 semanas

**DescripciÃ³n:**
Agregar modelos realistas de slippage y market impact al entorno de trading.

**RazÃ³n:**
- MVP usa simplificaciones (ejecuciÃ³n instantÃ¡nea a mid price)
- En producciÃ³n, trades tienen slippage y mueven el mercado
- Resultados de backtest serÃ­an optimistas sin esto

**ImplementaciÃ³n:**
```python
class RealisticExecutionModel:
    def execute_trade(self, size, current_spread, recent_volume):
        # Modelo de slippage
        base_slippage = self.spread_slippage(current_spread)
        volume_slippage = self.volume_slippage(size, recent_volume)
        
        # Market impact (proporcional a tamaÃ±o relativo)
        market_impact = self.calculate_impact(size, recent_volume)
        
        total_cost = base_slippage + volume_slippage + market_impact
        return total_cost
```

**Referencias:**
- Almgren & Chriss (2000) - Optimal Execution
- Gatheral (2010) - Market Impact Models

---

### 3. Sistema de GestiÃ³n de Riesgo en Tiempo Real

**Status:** ğŸ“‹ Backlog  
**CategorÃ­a:** Risk Management  
**Esfuerzo Estimado:** 2-3 semanas

**DescripciÃ³n:**
Implementar sistema de lÃ­mites y circuit breakers para producciÃ³n.

**Componentes:**
- **Stop-loss dinÃ¡mico:** Basado en volatilidad reciente
- **Position limits:** MÃ¡ximo por par, mÃ¡ximo total
- **Drawdown limits:** Pausa trading si drawdown > threshold
- **Exposure limits:** Balance long/short
- **Velocity checks:** Detectar comportamiento anormal del agente

**ImplementaciÃ³n:**
```python
class RiskManager:
    def check_trade_allowed(self, action, portfolio_state):
        # Check position limits
        if self.exceeds_position_limit(action, portfolio_state):
            return False, "Position limit exceeded"
        
        # Check drawdown threshold
        if self.in_drawdown_protection(portfolio_state):
            return False, "Drawdown protection active"
        
        # Check exposure limits
        if self.exceeds_exposure_limit(action, portfolio_state):
            return False, "Exposure limit exceeded"
        
        return True, None
```

**ParÃ¡metros Sugeridos:**
- Max position per pair: 10% del capital
- Max total exposure: 30% del capital
- Drawdown threshold: -15% desde peak
- Daily loss limit: -5% del capital

---

### 4. Deployment Pipeline (ProducciÃ³n)

**Status:** ğŸ“‹ Backlog  
**CategorÃ­a:** MLOps  
**Esfuerzo Estimado:** 3-4 semanas

**DescripciÃ³n:**
Sistema completo de deployment para trading en vivo.

**Arquitectura:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Ingestion â”‚  â† WebSocket feed (Dukascopy/broker API)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Engine  â”‚  â† Real-time pipeline (mismas features que entrenamiento)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Serving  â”‚  â† VAE â†’ TFT â†’ SAC (TorchScript optimizado)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Risk Manager   â”‚  â† ValidaciÃ³n de trades
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Order Execution â”‚  â† Broker API (OANDA, Interactive Brokers)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Monitoring    â”‚  â† Prometheus + Grafana dashboards
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**TecnologÃ­as:**
- **Serving:** FastAPI + TorchScript
- **ContainerizaciÃ³n:** Docker + Docker Compose
- **OrquestaciÃ³n:** Kubernetes (opcional, para alta disponibilidad)
- **Monitoring:** Prometheus + Grafana
- **Logging:** ELK Stack o Loki

**MÃ©tricas a Monitorear:**
- Latencia de predicciÃ³n (P50, P95, P99)
- Throughput (predicciones/segundo)
- Accuracy de pronÃ³sticos vs. realidad
- PnL en tiempo real
- Drawdown actual vs. histÃ³rico
- Tasa de errores (fallos de API, timeouts)

---

## â­ VersiÃ³n 2.0 (Media Prioridad)

### 5. Multi-Agent Benchmarking Framework

**Status:** ğŸ“‹ Backlog  
**CategorÃ­a:** ExperimentaciÃ³n  
**Esfuerzo Estimado:** 2-3 semanas

**DescripciÃ³n:**
Framework para comparar SAC contra otros algoritmos de RL en mismas condiciones.

**Agentes a Benchmarking:**
1. **SAC (Baseline):** Nuestro agente principal
2. **TD3 (Twin Delayed DDPG):** MÃ¡s estable, menos exploraciÃ³n
3. **PPO (Proximal Policy Optimization):** On-policy, mÃ¡s sample-efficient
4. **DQN:** Baseline discreto (si discretizamos acciones)
5. **A2C/A3C:** Actor-Critic simple

**Por QuÃ© Cada Uno:**

**TD3:**
- âœ… Pros: Muy estable, menos hiperparÃ¡metros sensibles
- âŒ Contras: PolÃ­tica determinista (menos exploraciÃ³n)
- ğŸ¯ Uso: Comparar si la exploraciÃ³n estocÃ¡stica de SAC ayuda

**PPO:**
- âœ… Pros: On-policy (mejor convergencia), mÃ¡s robusto
- âŒ Contras: Menos sample-efficient, mÃ¡s lento
- ğŸ¯ Uso: Ver si on-policy es mejor para markets (no-estacionarios)

**RazÃ³n del Framework:**
- Validar que SAC es la mejor elecciÃ³n
- Detectar si hay regÃ­menes de mercado donde otros agentes funcionan mejor
- Publicar resultados en paper/blog (credibilidad cientÃ­fica)

**ImplementaciÃ³n:**
```python
# experiments/benchmark_agents.py
agents = {
    'SAC': SACAgent(config),
    'TD3': TD3Agent(config),
    'PPO': PPOAgent(config),
}

results = {}
for name, agent in agents.items():
    print(f"Training {name}...")
    agent.train(env, n_episodes=10000)
    
    print(f"Evaluating {name}...")
    metrics = evaluate(agent, test_env, n_episodes=100)
    results[name] = metrics
    
# Comparar mÃ©tricas
compare_agents(results)  # Genera tablas y grÃ¡ficos
```

**MÃ©tricas de ComparaciÃ³n:**
- Sharpe Ratio (risk-adjusted returns)
- Convergence speed (episodes to threshold)
- Sample efficiency (returns per sample)
- Stability (variance across seeds)
- Robustness (performance en diferentes periodos)

**Entregables:**
1. Script de benchmark automatizado
2. Resultados en formato tabla (LaTeX para papers)
3. GrÃ¡ficos de comparaciÃ³n (learning curves, violin plots)
4. Documento tÃ©cnico con anÃ¡lisis

---

### 6. Hierarchical RL (Meta-Policy)

**Status:** ğŸ”¬ InvestigaciÃ³n  
**CategorÃ­a:** Algoritmos  
**Esfuerzo Estimado:** 4-6 semanas

**DescripciÃ³n:**
Implementar arquitectura jerÃ¡rquica donde un meta-agente selecciona estrategias y sub-agentes las ejecutan.

**MotivaciÃ³n:**
- Markets tienen regÃ­menes (trending, ranging, high volatility)
- Un solo agente puede no ser Ã³ptimo en todos los regÃ­menes
- Meta-learner puede aprender a cambiar de estrategia

**Arquitectura:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Meta-Policy (PPO)            â”‚
â”‚  Input: Market regime features       â”‚
â”‚  Output: Strategy selection (0-N)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
       â”‚               â”‚       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Strategy 1 â”‚ â”‚ Strategy 2 â”‚ â”‚ Strategy 3 â”‚
â”‚ (Trend)    â”‚ â”‚ (Mean Rev) â”‚ â”‚ (Breakout) â”‚
â”‚ SAC Agent  â”‚ â”‚ SAC Agent  â”‚ â”‚ SAC Agent  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplementaciÃ³n:**
1. Pre-entrenar sub-agentes en regÃ­menes especÃ­ficos
2. Entrenar meta-policy para seleccionar sub-agente
3. Opcional: Fine-tune end-to-end

**MÃ©tricas:**
- Â¿Mejora sobre single-agent SAC?
- Â¿Correctness de regime detection?
- Â¿Switching cost (transaction costs al cambiar estrategia)?

**Referencias:**
- Bacon et al. (2017) - The Option-Critic Architecture
- Vezhnevets et al. (2017) - FeUdal Networks

---

### 7. Offline RL (Batch RL)

**Status:** ğŸ”¬ InvestigaciÃ³n  
**CategorÃ­a:** Algoritmos  
**Esfuerzo Estimado:** 3-4 semanas

**DescripciÃ³n:**
Entrenar agente puramente en datos histÃ³ricos sin interacciÃ³n con environment.

**MotivaciÃ³n:**
- MÃ¡s sample-efficient (usa todo el histÃ³rico disponible)
- No requiere simulaciÃ³n durante entrenamiento
- Evita exploration noise en training

**Algoritmos Candidatos:**
- **BCQ (Batch-Constrained Q-learning)**
- **CQL (Conservative Q-Learning)**
- **TD3+BC** (TD3 with Behavior Cloning)

**Workflow:**
1. Generar dataset de experiencias (offline dataset)
2. Entrenar agente offline
3. Evaluar en environment
4. Comparar vs. SAC online

**Ventajas:**
- Entrena mÃ¡s rÃ¡pido (no espera simulaciÃ³n)
- Usa mÃ¡s data histÃ³rica
- MÃ¡s estable (no exploration randomness)

**Desventajas:**
- Puede sufrir de distribution shift
- Requiere dataset de buena calidad
- Menos exploraciÃ³n

---

### 8. Explainability & Interpretability

**Status:** ğŸ“‹ Backlog  
**CategorÃ­a:** AnÃ¡lisis  
**Esfuerzo Estimado:** 2-3 semanas

**DescripciÃ³n:**
Agregar herramientas para entender por quÃ© el agente toma decisiones.

**Componentes:**

**1. Attention Weights Visualization (TFT)**
```python
# Visualizar quÃ© features atiende el TFT
def plot_attention_weights(tft_model, sample_input):
    attention_weights = tft_model.get_attention_weights(sample_input)
    plt.imshow(attention_weights, cmap='viridis')
    plt.ylabel('Time Step')
    plt.xlabel('Feature')
    plt.show()
```

**2. SHAP Values (Feature Importance)**
```python
import shap

# Explicar decisiÃ³n del agente
explainer = shap.DeepExplainer(sac_agent.actor, background_data)
shap_values = explainer.shap_values(current_state)

shap.waterfall_plot(shap_values)
```

**3. Policy Rollouts Visualization**
- Mostrar estados visitados en latent space (VAE)
- Colorear por reward obtenido
- Identificar regiones de alta/baja performance

**4. Trade Analysis Dashboard**
- Â¿QuÃ© features tenÃ­an valores altos cuando se hizo buen trade?
- Â¿CuÃ¡ndo fallÃ³ el agente? (post-mortem analysis)
- Â¿CorrelaciÃ³n entre forecast uncertainty y trading decisions?

**RazÃ³n:**
- Debugging (entender errores del modelo)
- Confianza (stakeholders quieren explicaciones)
- Mejora (insights para feature engineering)

---

### 9. Walk-Forward Optimization

**Status:** ğŸ“‹ Backlog  
**CategorÃ­a:** ValidaciÃ³n  
**Esfuerzo Estimado:** 2-3 semanas

**DescripciÃ³n:**
ValidaciÃ³n robusta mediante re-entrenamiento periÃ³dico en ventanas mÃ³viles.

**MetodologÃ­a:**
```
Train: [2020-01-01 to 2020-12-31]
Test:  [2021-01-01 to 2021-03-31]

Train: [2020-04-01 to 2021-03-31]
Test:  [2021-04-01 to 2021-06-30]

Train: [2020-07-01 to 2021-06-30]
Test:  [2021-07-01 to 2021-09-30]

...
```

**Ventajas:**
- Simula re-entrenamiento en producciÃ³n
- Detecta degradaciÃ³n de performance over time
- EvalÃºa robustness a cambios de mercado

**MÃ©tricas:**
- Sharpe ratio promedio en todos los folds
- Peor fold (worst-case scenario)
- Stability (std dev de Sharpe across folds)

**ImplementaciÃ³n:**
```python
def walk_forward_validation(agent_class, data, train_window, test_window):
    results = []
    
    for start in range(0, len(data) - train_window - test_window, test_window):
        train_data = data[start:start+train_window]
        test_data = data[start+train_window:start+train_window+test_window]
        
        # Train agent
        agent = agent_class()
        agent.train(train_data)
        
        # Evaluate
        metrics = agent.evaluate(test_data)
        results.append(metrics)
    
    return pd.DataFrame(results)
```

---

### 10. Portfolio Optimization (Multi-Asset Trading)

**Status:** ğŸ“‹ Backlog  
**CategorÃ­a:** Trading Strategy  
**Esfuerzo Estimado:** 3-4 semanas

**DescripciÃ³n:**
Optimizar portafolio multi-asset en vez de tradear pares independientemente.

**Actualmente:**
- SAC decide posiciÃ³n para cada par de forma independiente
- No considera correlaciones entre pares

**Mejora:**
- Usar Markowitz Mean-Variance Optimization
- O agregar peso de diversificaciÃ³n en reward function
- O entrenar SAC con action space = portfolio weights

**Reward Function Actualizada:**
```python
def portfolio_reward(actions, returns, cov_matrix):
    # Expected return
    portfolio_return = actions @ returns
    
    # Portfolio variance (penalizar alta correlaciÃ³n)
    portfolio_variance = actions @ cov_matrix @ actions
    
    # Sharpe ratio (return per unit of risk)
    sharpe = portfolio_return / np.sqrt(portfolio_variance)
    
    return sharpe
```

**Ventajas:**
- Mejor diversificaciÃ³n
- Menor riesgo total
- Mayor Sharpe ratio

---

## ğŸ’¡ Futuro (Baja Prioridad)

### 11. Alternative Data Integration

**Status:** ğŸ’¡ Idea  
**CategorÃ­a:** Datos

**Fuentes de Datos Alternativas:**
- **Sentiment Analysis:** Twitter, Reddit, news headlines
- **Order Book Data:** Level 2+ (profundidad de mercado)
- **Economic Calendar:** Eventos macro (NFP, Fed meetings, etc.)
- **Options Flow:** Indicador de sentimiento institucional

**RazÃ³n:**
- Potencial edge sobre competencia
- Mejor captura de rÃ©gimen de mercado

**DesafÃ­os:**
- Requiere licencias caras (Bloomberg, Refinitiv)
- Noisy data (especialmente sentiment)
- MÃ¡s complejidad en pipeline

---

### 12. Adversarial Training

**Status:** ğŸ”¬ InvestigaciÃ³n  
**CategorÃ­a:** Robustness

**DescripciÃ³n:**
Entrenar agente contra adversary que intenta explotar sus debilidades.

**MotivaciÃ³n:**
- Markets son adversariales (otros traders, HFTs)
- Agente debe ser robusto a manipulaciÃ³n

**ImplementaciÃ³n:**
```python
# Adversarial environment
class AdversarialMarket(TradingEnv):
    def __init__(self, agent, adversary):
        self.agent = agent
        self.adversary = adversary
    
    def step(self, action):
        # Adversary intenta predecir acciÃ³n del agente
        adversary_action = self.adversary.predict(self.state)
        
        # Modifica mercado para castigar al agente
        market_impact = self.apply_adversarial_impact(adversary_action)
        
        return self._execute_with_impact(action, market_impact)
```

**Referencias:**
- Pinto et al. (2017) - Robust Adversarial Reinforcement Learning
- Gleave et al. (2020) - Adversarial Policies

---

### 13. Meta-Learning (Learning to Learn)

**Status:** ğŸ”¬ InvestigaciÃ³n  
**CategorÃ­a:** Algoritmos

**DescripciÃ³n:**
Entrenar agente que se adapta rÃ¡pidamente a nuevos regÃ­menes de mercado.

**Algoritmos:**
- **MAML (Model-Agnostic Meta-Learning)**
- **Reptile**

**MotivaciÃ³n:**
- Markets cambian constantemente
- Agente que aprende rÃ¡pido tiene ventaja

**Workflow:**
1. Meta-train en mÃºltiples periodos histÃ³ricos (tasks)
2. Fine-tune en nuevo periodo con pocos samples
3. Evaluar adaptaciÃ³n speed

---

### 14. Ensemble Methods

**Status:** ğŸ’¡ Idea  
**CategorÃ­a:** Modelos

**DescripciÃ³n:**
Combinar mÃºltiples modelos SAC para mejorar robustness.

**Tipos de Ensemble:**

**1. Model Averaging:**
```python
actions = [agent.predict(state) for agent in ensemble]
final_action = np.mean(actions, axis=0)
```

**2. Voting:**
- Cada agente vota por acciÃ³n
- AcciÃ³n con mÃ¡s votos se ejecuta

**3. Stacking:**
- Meta-learner combina predicciones de mÃºltiples agentes

**Ventajas:**
- Reduce variance
- MÃ¡s robusto a overfitting

**Desventajas:**
- MÃ¡s lento (N veces mÃ¡s inferencias)
- MÃ¡s complejo de mantener

---

## âŒ Ideas Descartadas

### ~~Reinforcement Learning from Human Feedback (RLHF)~~

**RazÃ³n para Descartar:**
- No tenemos expertos disponibles para labeling
- Too expensive and time-consuming
- MVP debe ser fully automated

---

### ~~Multi-Task Learning (MTL)~~

**RazÃ³n para Descartar:**
- No tenemos tareas relacionadas obvias
- Forex trading es suficientemente complejo por sÃ­ solo
- MTL agrega complejidad sin clear benefit

---

## ğŸ”„ Proceso de GestiÃ³n del Backlog

### AÃ±adir Items

1. Escribir descripciÃ³n clara
2. Asignar categorÃ­a y prioridad
3. Estimar esfuerzo
4. Agregar razÃ³n/motivaciÃ³n
5. Opcional: Referencias o diseÃ±o preliminar

### PriorizaciÃ³n

Cada trimestre, revisar y re-priorizar basado en:
- Feedback de MVP en producciÃ³n
- Nuevos papers/tÃ©cnicas
- Recursos disponibles
- Impacto estimado en performance

### Completar Items

Al implementar:
1. Crear issue en GitHub
2. Implementar en feature branch
3. PR con tests y documentaciÃ³n
4. Actualizar backlog: âœ… Completado

---

## PrÃ³ximos Pasos

### Inmediato (Post-MVP)
1. âœ… Completar MVP baseline (en progreso)
2. ğŸ”„ Evaluar performance del MVP
3. ğŸ“‹ Decidir cuÃ¡les items del backlog son prioritarios
4. ğŸš€ Comenzar implementaciÃ³n de Post-MVP features

### RevisiÃ³n del Backlog
- **Frecuencia:** Mensual
- **Owner:** Tech Lead / Product Owner
- **Criterios:** Performance metrics, ROI, effort vs. impact

---

## Contribuir al Backlog

Si tienes ideas para agregar:
1. Crear issue en GitHub con label `backlog-idea`
2. Usar template:
   ```
   **TÃ­tulo:** [Nombre descriptivo]
   **CategorÃ­a:** [Datos/Algoritmos/MLOps/etc.]
   **Prioridad Sugerida:** [Alta/Media/Baja]
   **DescripciÃ³n:** [Â¿QuÃ© quieres implementar?]
   **RazÃ³n:** [Â¿Por quÃ© es valioso?]
   **Esfuerzo Estimado:** [Semanas]
   ```

---

**Autor:** AtlasFX Team  
**Ãšltima ActualizaciÃ³n:** Octubre 2025  
**VersiÃ³n:** 1.0
